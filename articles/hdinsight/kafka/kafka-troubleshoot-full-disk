---
title: Broker fails to start due to a full disk in Azure HDInsight
description: Troubleshooting steps for Apache Kafka broker process that does not cannot start due to disk full error. 
ms.service: hdinsight
ms.topic: troubleshooting
ms.date: 10/08/2021
---

# Scenario: Brokers are unhealthy or unable to restart due to disk space full issue

This article describes troubleshooting steps and possible resolutions for issues when interacting with Azure HDInsight clusters.

## Issue

The data disks used by Apache Kafka brokers in Azure HDInsight clusters can fill up. When that happens, the Apache Kafka broker process cannot start since it fails due to the disk full error. If you have made any recent configuration changes, those changes do not take into effect since the Kafka broker process doesn't start.

## Cause

- You cannot increase the number of disks or increase the disk size once we have created the Kafka cluster.
- Usually, disk alerts are configured in the Apache Ambari UI. Whenever the usage of disk increases from 60%, it gives an alert that we need to either scale out or reduce the amount of logs present in Kafka cluster.
- This issue will happen only if you ignore the disk alerts. You could scale out the cluster to make more space available to respond to the disk space alerts.
- Messages are not immediately deleted even though the retention time passes. Messages to be deleted will have a mark of deletion. Then later background cleanup process will delete messages. Also messages in only passive segments are deleted.

> [!IMPORTANT]
> There are several configs to enhance the log cleaner performance but please be very CAREFUL to apply this enhancement since it may affect producing/consuming messages

## Mitigation

1. Check the `server.properties` files to find the retention time for every topic. Sometimes the log retention policy is set at topic level. To find retention time configured at topic level, run the following command:

```bash
/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --describe --zookeeper <zookeeper-list> 


2. Once you have that information, check which heavy partitions occupy the maximum disk space.

```bash
# Command to sort the directories based on size:
du -hs * | sort -rh | head -5 
 
3. Backup the files which are older than the new retention time.
4. When there is some free space available, you can restart the brokers with the new retention time configuration. This will clean the older logs and free up some space on disk.
 
> [!IMPORTANT]
> Sometimes taking a backup might not be an option because either OS disk might get full or just overload other kafka disks, in that case we might have to delete the files older than the retention time.

## Resolution

Even though you can decrease the retention time, it will not be scalable if you want to add more topics in the cluster or if the load or amount of data ingested increases.
To avoid the above the scenario these are the following options:

- If the partitions are too big, increase the number of partitions for the heaviest topics.

   > [!NOTE] 
   > But increasing the partitions in an existing topic doesn't reorganize the data in the topic. You have to manually copy data from a old low partition topic to a new higher partition topic in the first. 
- Scale out the cluster and re-balance all the partitions across multiple disks.

- Create a cluster with bigger sku and more no. of disks attached.

## Next steps
[!INCLUDE [troubleshooting next steps](../includes/hdinsight-troubleshooting-next-steps.md)]
