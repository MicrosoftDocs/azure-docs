---
title: Images in Cognitive Search | Microsoft Docs
description: Dealing with Images in Cognitive Search
services: search
manager: pablocas
author: luiscabrer
documentationcenter: ''

ms.assetid: 
ms.service: search
ms.devlang: NA
ms.workload: search
ms.topic: article
ms.tgt_pltfrm: na
ms.date: 05/01/2018
ms.author: heidist
---
# Dealing with Images for Cognitive Search Scenarios

Cognitive Search has several capabilities that allow you to work with images and image files.

## Getting Normalized Images

As part of the "document cracking" step, there is a new set of indexer configuration parameters. These parameters allow you to specify what an indexer should do when it encounters image files or images that are embedded in files; for instance, a .PDF that contains several images inside it.

| Configuration Parameter | Description |
|--------------------|-------------|
| imageAction	| Set to "none" if no action should be taken when embedded images or image files are encountered. <br/>Set to either "generateNormalizedImages" to generate an array of normalized images as part of document cracking. These images will be exposed in the *normalized_images* field. <br/>The default is "none." This configuration is only pertinent to blob data sources, when "dataToExtract" is set to "contentAndMetadata." |
|  normalizedImageMaxWidth | The maximum width (in pixels) for normalized images generated. The default is 2000.|
|  normalizedImageMaxHeight | The maximum height (in pixels) for normalized images generated. The default is 2000.|


The default of 2000 pixels for the normalized images maximum width and height is based on the maximum sizes supported by the [OCR skill](cognitive-search-skill-ocr.md) and the [image analysis skill](cognitive-search-skill-image-analysis.md). 


You specify the imageAction in your [indexer definition](ref-create-indexer.md) as follows:

```json
{
  //...rest of your indexer definition goes here ...
  "parameters":
  {
    "configuration": 
    {
    	"dataToExtract": "contentAndMetadata",
     	"imageAction": "generateNormalizedImages"
    }
  }
}
```

When the *imageAction* is set to "generateNormalizedImages", the new *normalized_images* field will contain an array of images. Each  image is a complex type that has the following members:

| Image member       | Description                             |
|--------------------|-----------------------------------------|
| data               | BASE64 encoded string of the normalized image in JPEG format.   |
| width              | Width of the normalized image in pixels. |
| height             | Height of the normalized image in pixels. |
| originalWidth      | The original width of the image before normalization. |
| originalHeight      | The original height of the image before normalization. |
| rotationFromOriginal |  Counter-clockwise rotation in degrees that occurred to create the normalized image. A value between 0 degrees and 360 degrees. This step reads the metadata from the image that is generated by a camera or scanner. Usually a multiple of 90 degrees. |
| contentOffset |The character offset within the content field where the image was extracted from. This field is only applicable for files with embedded images. |

 Sample value of *normalized_images*:
```json
[
  {
    "data": "BASE64 ENCODED STRING OF A JPEG IMAGE",
    "width": 500,
    "height": 300,
    "originalWidth": 5000,  
    "originalHeight": 3000,
    "rotationFromOriginal": 90,
    "contentOffset": 500  
  }
]
```

## Skills to deal with images

Currently there are two built-in cognitive skills that take images as an input, the OCR skill and the analyze image skill. 

At this point, the *analyze image skill* and the *OCR skill* only work with images generated from the document cracking step. So the only supported input is "\document\normalized_images".

## Analyze Image Skill

The image analysis skill extracts a rich set of visual features based on the image content. For instance, you can generate a caption from an image, generate tags, or identify celebrities and landmarks.

Learn more about it [here](cognitive-search-skill-image-analysis.md).

## OCR Skill

This skill extracts text from image files such as JPGs, PNGs, and bitmaps. It can extract text as well as layout information. The layout  information provides bounding boxes for each of the strings identified.

The OCR skill allows you to select the algorithm to use for detecting text in your images. Currently it supports two algorithms, one for printed text and another on for handwritten text.

Learn more about it [here](cognitive-search-skill-ocr.md).


## Common Scenario: Dealing with files that contain embedded images

If you need to create a single string that includes all the content of the file, you need to perform the following steps:  

1. Extract normalized_images. (as described earlier in this document)
1. OCR those images
1. Merge the text representation of those images with the text content extracted. You can use the [Text Merge](cognitive-search-skill-textmerger.md) skill to do that.

The following example skillset creates a *merged_text* field to contain the textual content of your document. It also includes the OCRed text from each of the embedded images. 

#### Request Body Syntax
```json
{
  "description": "Extract text from images and merge with content text to produce merged_text",
  "skills":
  [
    {
        "name": "OCR skill",
        "description": "Extract text (plain and structured) from image.",
        "@odata.type": "#Microsoft.Skills.Vision.OcrSkill",
        "context": "/document/normalized_images/*",
        "defaultLanguageCode": "en",
        "detectOrientation": true,
        "inputs": [
          {
            "name": "image",
            "source": "/document/normalized_images/*"
          }
        ],
        "outputs": [
          {
            "name": "text"
          }
        ]
    },
    {
      "@odata.type": "#Microsoft.Skills.Text.MergeSkill",
      "description": "Create merged_text, which includes all the textual representation of each image inserted at the right location in the content field.",
      "context": "/document",
      "insertPreTag": " ",
      "insertPostTag": " ",
      "inputs": [
        {
          "name":"text", "source": "/document/content"
        },
        {
          "name": "itemsToInsert", "source": "/document/normalized_images/*/text"
        },
        {
          "name":"offsets", "source": "/document/normalized_images/*/contentOffset" 
        }
      ],
      "outputs": [
        {
          "name": "mergedText", "targetname" : "merged_text"
        }
      ]
    }
  ]
}
```

Now that you have a merged_text field, you could map that as a searchable field in your indexer definition. All of the content of your files, including the text of the images, will be searchable!

## Common Scenario: Visualizing bounding boxes of extracted text

Sometimes you need to visualize search results layout information. For instance, you may want to highlight where a piece of text is found in an image as part of your search results.

Since the OCR step is taking place on the normalized images, the layout coordinates will be in the normalized image space. If you display the normalized image that would not be a problem, but there will be situations where you may want to display the original image. In that case convert each of coordinate points in the layout to the original image coordinate system. 

As a helper, if you need to transform normalized coordinates to the original coordinate space, you could use the following algorithm:
```csharp
        /// <summary>
        ///  Converts a point in the normalized coordinate space to the original coordinate space.
        ///  This method assumes the rotation angles are multiples of 90 degrees.
        /// </summary>
        public static Point GetOriginalCoordinates(Point normalized,
                                    int originalWidth,
                                    int originalHeight,
                                    int width,
                                    int height,
                                    double rotationFromOriginal)
        {
            Point original = new Point();
            double angle = rotationFromOriginal % 360;

            if (angle == 0 )
            {
                original.X = normalized.X;
                original.Y = normalized.Y;
            } else if (angle == 90)
            {
                original.X = normalized.Y;
                original.Y = (width - normalized.X);
            } else if (angle == 180)
            {
                original.X = (width -  normalized.X);
                original.Y = (height - normalized.Y);
            } else if (angle == 270)
            {
                original.X = height - normalized.Y;
                original.Y = normalized.X;
            }

            double scalingFactor = (angle % 180 == 0) ? originalHeight / height : originalHeight / width;
            original.X = (int) (original.X * scalingFactor);
            original.Y = (int)(original.Y * scalingFactor);

            return original;
        }
```

## See also
+ [Create indexer (REST)](ref-create-indexer.md)
+ [Analyze image skill](cognitive-search-skill-image-analysis.md)
+ [OCR skill](cognitive-search-skill-ocr.md)
+ [Text merge skill](cognitive-search-skill-textmerger.md)
+ [How to define a skillset](cognitive-search-defining-skillset.md)
+ [How to map enriched fields](cognitive-search-output-field-mapping.md)