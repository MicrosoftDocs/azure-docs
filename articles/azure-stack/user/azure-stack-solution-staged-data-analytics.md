---
title: Create a staged data analytics solution with Azure and Azure Stack | Microsoft Docs
description: Learn how to Create a staged data analytics solution with Azure and Azure Stack.
services: azure-stack
documentationcenter: ''
author: mattbriggs
manager: femila
editor: ''

ms.service: azure-stack
ms.workload: na
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: tutorial
ms.date: 09/24/2018
ms.author: mabrigg
ms.reviewer: Anjay.Ajodha
---

# Tutorial: Create a staged data analytics solution with Azure and Azure Stack 

*Applies to: Azure Stack integrated systems and Azure Stack Development Kit*

Learn how to configure a hybrid cloud identity for your Azure and Azure Stack applications.

Utilizing both on-premises and public cloud environments meets the demands of multi-facility enterprises. Azure Stack offers a rapid, secure, and flexible solution for collecting, processing, storing and distributing local and remote data, particularly when security, confidentiality, corporate policy, and regulatory requirements may differ between locations and users.

In this tutorial, you will build a sample environment to:

> [!div class="checklist"]
> - Create the raw data storage blob.
> - Create a New Azure Stack Function to move clean data from Azure Stack to Azure.
> - Create a Blob storage triggered function.
> - Create an Azure Stack storage account containing a blob and a queue.
> - Create a queue triggered function.
> - Test the queue triggered function.

> [!Tip]  
> ![hybrid-pillars.png](./media/azure-stack-solution-cloud-burst/hybrid-pillars.png)  
> Microsoft Azure Stack is an extension of Azure. Azure Stack brings the agility and innovation of cloud computing to your on-premises environment and enabling the only hybrid cloud that allows you to build and deploy hybrid apps anywhere.  
> 
> The whitepaper [Design Considerations for Hybrid Applications](https://aka.ms/hybrid-cloud-applications-pillars) reviews pillars of software quality (placement, scalability, availability, resiliency, manageability and security) for designing, deploying and operating hybrid applications. The design considerations assist in optimizing hybrid application design, minimizing challenges in production environments.


## Prerequisites

Some preparation is required to build this use case:

-   An installed and functioning Azure Stack (more information can be found here: [Azure Stack overview)](https://docs.microsoft.com/azure/azure-stack/user/azure-stack-storage-overview)

-   An Azure subscription. (Create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F))

-   Download and install the [Microsoft Azure Storage Explorer](http://storageexplorer.com/).

-   The data processed by these functions is not provided. Data must be generated and available to upload to the Azure Stack storage blob container.

## Issues and Considerations

### Scalability considerations

Azure functions and storage solutions scale to meet data volume and processing demands. For Azure scalability information and targets, see [Azure scalability documentation](https://docs.microsoft.com/azure/storage/common/storage-scalability-targets).

### Availability considerations

Storage is the primary availability consideration for this pattern. Connection via fast links is required for large data volume processing and distribution.

### Manageability considerations

Manageability of this solution depends on authoring tools in use and engagement of source control.

## Create the raw data storage blob

The storage account and blob container will hold all original data generated by on-premises activities, including machine and employee activity, facility data, production metrics, and other reporting.

1.  Sign in to the[*Azure Stack portal*](https://portal.local.azurestack.external/).

2.  In the Azure Stack portal, expand the menu on the left side to open the menu of services and choose **All Services**. Scroll down to**Storage** and choose **Storage accounts**. In the Storage Accounts window, choose **Add**.

3.  Use the following information for the account:

    a.  Name: **Your choice**

    b.  Deployment model: **Resource Manager**

    c.  Account kind: **Storage (general-purpose V1)**

    d.  Location: **West US**

    e.  Replication: **Locally redundant storage (LRS)**

    f.  Performance: **Standard**

    g.  Secure transfer required: **Disabled**

    h.  Subscription: Choose one

    i.  Resource group: Specify a new resource group or select an     existing resource group

    j.  Configure virtual networks: **Disabled**

4.  Select **Create to create the storage account.

  ![Alt text](media\azure-stack-solution-staged-data-analytics\image1.png)

1.  Once created, select the name of the storage account.

2.  In the account blade, under the **BLOB SERVICE** heading, select **Containers**.

3.  At the top of the blade, select**+ Container.**and select**Container**.

  ![Alt text](media\azure-stack-solution-staged-data-analytics\image2.png){width="3.933333333333333in" height="2.3952996500437447in"}

1.  Name: **Your Choice**

2.  Public access level: **Container** (anonymous read access for containers and blobs)

3.  Select **OK**.

## Create a New Azure Stack Function to move clean data from Azure Stack to Azure

1.  Create a new Function by clicking on **Functions**, then the **+New Function** button.

  ![Alt text](media\azure-stack-solution-staged-data-analytics\image3.png)

2.  Select **Timer Trigger**.

  ![Alt text](media\azure-stack-solution-staged-data-analytics\image4.png)

3.  Select **C\#** as the Language and name the Function:** upload-to-azure**and set the Schedule to **0 0 \*/1 \* \* \*, which in CRON notation is once an hour.

  ![Alt text](media\azure-stack-solution-staged-data-analytics\image5.png)

4.  Create the function app using the settings specified in the table below the image.

  | Setting | Suggested value | Description |
  | ---- | ---- | ---- |
  | App name | Globally unique name | Name that identifies your new function app. Valid characters are a-z, 0-9, and -. |
  | Subscription | Your subscription | The subscription under which this new function app is created. |
  | Resource Group |  |  |
  | myResourceGroup | Name for the new resource group in which to create your function app. |  |
  | OS | Windows | Serverless hosting is currently only available when running on Windows. |
  | Hosting plan |  |  |
  | Consumption plan | Hosting plan that defines how resources are allocated to your function app. In the default Consumption Plan, resources are added dynamically as required by your functions. In this serverless hosting, you only pay for the time your functions run. |  |
  | Location | Region nearest you | Choose a region near you or near other services your functions access. |
  | Storage account |  |  |
  | \<storage account created above> | Name of the new storage account used by your function app. Storage account names must be between 3 and 24 characters in length and may contain numbers and lowercase letters only. You can also use an existing account. |  |

  **Example:**

  ![Define new function app settings](media\azure-stack-solution-staged-data-analytics\image6.png)

4.  Select **Create** to provision and deploy the function app.

5.  Select the Notification icon in the upper-right corner of the portal and watch for the **Deployment succeeded** message.

    ![Define new function app settings](media\azure-stack-solution-staged-data-analytics\image7.png)

4.  Select **Go to resource** to view new function app.

![Function app successfully created.](media\azure-stack-solution-staged-data-analytics\image8.png)

## Create a Blob storage triggered function

1.  Expand the function app and select the **+** button next to **Functions**. If this is the first function in the function app, select **Custom function**. This displays the complete set of function templates.

  ![Functions quickstart page in the Azure portal](media\azure-stack-solution-staged-data-analytics\image9.png)

1.  In the search field, type blob and then choose the desired language for the Blob storage trigger template.

  ![Choose the Blob storage trigger template.](media\azure-stack-solution-staged-data-analytics\image10.png)

1.  Use the settings as specified in the table below:

  | Setting | Suggested value | Description |
  | ------- | ------- | ------- |
  | Name | Unique in your function app | Name of this blob triggered function. |
  | Path | \<path from the storage location above> | Location in Blob storage being monitored. The file name of the blob is passed in the binding as the name parameter. |
  | Storage account connection | Function App Connection | You can use the storage account connection already being used by your function app or create a new one. |

  **Example:**

  ![Create the Blob storage triggered function.](media\azure-stack-solution-staged-data-analytics\image11.png)

1.  Select **Create** to create the function.

### Test the function

1.  In the Azure portal, browse to the function. Expand the **Logs** at the bottom of the page and ensure log streaming is not paused.

2.  Open Storage Explorer and connect to the storage account created at the beginning of this section.

3.  Expand the storage account, **Blob containers**, and **the blob you created earlier**. Select**Upload** and then **Upload files.**

  ![Upload a file to the blob container.](media\azure-stack-solution-staged-data-analytics\image12.png)

1.  In the Upload files dialog box, select the Files field. Browse to a file on a local computer, such as an image file, select it and select **Open** and then **Upload**.

2.  Go back to function logs and verify the blob has been read.

  **Example:**

  ![View message in the logs.](media\azure-stack-solution-staged-data-analytics\image13.png)

## Create an Azure Stack storage account containing a blob and a queue

### Storage Blob  Data archiving

This storage account will house two containers. These containers are one blob used to hold archive data, and a queue used for the processing of data assigned for main office distribution.

Use the steps and settings outlined above to create another storage account and blob container as our archive storage.

### Storage Queue  Filtered Data holding

1.  Use the steps and settings outlined above to access the new storage account.

2.  In the Storage Account Overview section, select **Queue.**

3.  Select the **+ Queue** and fill-in **Name** field and fill-in a name for the new queue.

4.  Select **OK.**

![Alt text](media\azure-stack-solution-staged-data-analytics\image14.png)

![Alt text](media\azure-stack-solution-staged-data-analytics\image15.png)

## Create a queue triggered function

1.  Use the steps in the above function creation section to create an additional Azure Stack function with a queue trigger instead of a blob trigger.

2.  Use the settings as specified in the table below:

| Setting | Suggested value | Description |
| ------- | ------- | ------- |
| Name | Unique in your function app | Name of this queue triggered function. |
| Path | \<path from the storage location above> | Location in storage being monitored. The file name of the queue is passed in the binding as the name parameter. |
| Storage account connection | Function App Connection | You can use the storage account connection already being used by your function app or create a new one. |

1.  Select**Create** to create the function.

## Test the queue triggered function

1.  In the Azure portal, browse to the function. Expand the **Logs** at the bottom of the page and ensure log streaming is not paused.

2.  Open Storage Explorer and connect to the storage account created at the beginning of this section.

3.  Expand the storage account, **Blob containers**, and **the blob you created earlier**. Select**Upload** and then **Upload files.**

![Upload a file to the blob container.](media\azure-stack-solution-staged-data-analytics\image12.png)

1.  In the Upload files dialog box, select the Files field. Browse to a file on a local computer, such as an image file, select it and select **Open** and then **Upload**.

2.  Go back to function logs and verify the blob has been read.

  **Example:**

  ![View message in the logs.](media\azure-stack-solution-staged-data-analytics\image13.png)

## Securely stored and accessed compliant data

Global enterprise data is securely stored, processed, distributed, and accessed via Azure and Azure Stack Staged Data Analytics and custom endpoint directives. Remote office employee and machinery activities, facility data, and business metrics are continually compiled, stored, tested for compliance, and distributed according to company policy and regional regulation.

## Next steps
- To learn more about Azure Cloud Patterns, see [Cloud Design Patterns](https://docs.microsoft.com/azure/architecture/patterns).