---
title: Captioning with speech to text - Speech service
titleSuffix: Azure Cognitive Services
description: An overview of key concepts for captioning with speech to text.
services: cognitive-services
author: eric-urban
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 03/10/2022
ms.author: eur
---

# Captioning with speech to text

Use captioning with speech to text to transcribe the spoken words into text. Captioning can accompany real time or pre-recorded speech. 

Here are some common captioning scenarios:
- Online courses and instructional videos
- Sporting events
- Voice and video calls

This guide covers captioning for speech, but does not include speaker ID or sound effects.

The following are additional aspects to consider:
* Center captions horizontally on the screen, in a large and prominent font. 
* Let your audience know that captions are generated by an automated service.
* Consider how many words and lines can fit on the screen. 
* Learn about captioning protocols such as [SMPTE-TT](https://ieeexplore.ieee.org/document/7291854). 
* Consider output formats such as SRT (SubRip Subtitle) and WebVTT (Web Video Text Tracks).

> [!TIP]
> Try the [Azure Video Analyzer for Media](/azure/azure-video-analyzer/video-analyzer-for-media-docs/video-indexer-overview) as a demonstration of how you can get captions for videos that you upload. 

Whether you are showing captions in real time or with a recording, you can use the [Speech SDK](speech-sdk.md) to recognize speech and get transcriptions. You can also use the [Batch transcription API](batch-transcription.md) for pre-recorded video. This guide shows examples using the speech SDK.

## Caption and speech synchronization 

You'll want to synchronize captions with the audio track, whether it's done in real time or with a prerecording. 

The Speech service returns the offset and duration of the recognized speech. 

- **Offset**: Used to measure the relative position of the speech that is currently being recognized, from the time that you started speech recognition. Speech recognition does not necessarily start at the beginning of the audio track. Offset is measured in ticks, where a single tick represents one hundred nanoseconds or one ten-millionth of a second.
- **Duration**: Duration of the utterance that is being recognized. The duration time span does not include trailing or leading silence. 

As soon as you start continuous recognition, the offset starts incrementing in ticks from `0` (zero). 

```csharp
// Starts continuous recognition. Use StopContinuousRecognitionAsync() to stop recognition.
await speech_recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
```

With the `Recognizing` event, you can get the offset and duration of the speech being recognized. Offset and duration per word are not available while recognition is in progress. Each `Recognizing` event comes with a textual estimate of the speech recognized so far.

This code snippet shows how to get the offset and duration from a `Recognizing` event. 

```csharp
speech_recognizer.Recognizing += (object sender, SpeechRecognitionEventArgs e) =>
    {
        if (e.Result.Reason == ResultReason.RecognizingSpeech)
        {        
            Console.WriteLine($"RECOGNIZING: Text={e.Result.Text}");
            Console.WriteLine(String.Format ("Offset in Ticks: {0}", e.Result.OffsetInTicks));
            Console.WriteLine(String.Format ("Duration in Ticks: {0}", e.Result.Duration.Ticks));
        }
    };
```

Once an utterance has been recognized, you can get the offset and duration of the recognized speech. With the `Recognized` event, you can also get the offset and duration per word. To request the offset and duration per word, first you must set the corresponding `SpeechConfig` property as shown here:

```csharp
speechConfig.RequestWordLevelTimestamps();
```

This code snippet shows how to get the offset and duration from a `Recognized` event. 

```csharp
speech_recognizer.Recognized += (object sender, SpeechRecognitionEventArgs e) =>
    {
        if (ResultReason.RecognizedSpeech == e.Result.Reason && e.Result.Text.Length > 0)
        {            
            Console.WriteLine($"RECOGNIZED: Text={e.Result.Text}");
            Console.WriteLine(String.Format ("Offset in Ticks: {0}", e.Result.OffsetInTicks));
            Console.WriteLine(String.Format ("Duration in Ticks: {0}", e.Result.Duration.Ticks));
                        
            var detailedResults = speechRecognitionResult.Best();
            if(detailedResults != null && detailedResults.Any())
            {
                // The first item in detailedResults corresponds to the recognized text.
                // This is not necessarily the item with the highest confidence number.
                var bestResults = detailedResults?.ToList()[0];
                Console.WriteLine($"\tConfidence: {bestResults.Confidence}\n\tText: {bestResults.Text}\n\tLexicalForm: {bestResults.LexicalForm}\n\tNormalizedForm: {bestResults.NormalizedForm}\n\tMaskedNormalizedForm: {bestResults.MaskedNormalizedForm}");
                
                // You must set speechConfig.RequestWordLevelTimestamps() to get word-level timestamps.
                Console.WriteLine($"\tWord-level timing:");
                Console.WriteLine($"\t\tWord | Offset | Duration");
                Console.WriteLine($"\t\t----- | ----- | ----- ");

                foreach (var word in bestResults.Words)
                {
                    Console.WriteLine($"\t\t{word.Word} | {word.Offset} | {word.Duration}");
                }
            }
        }
    };
```

The following table shows potential offset and duration in ticks when a speaker says "Welcome to Applied Mathematics course 201." For each utterance, the offset doesn't change throughout the `Recognized` and `Recognized` events. The duration of speech recognized so far is calculated as an offset from the beginning of the utterance.

|Event  |Text  |Offset (in ticks)  |Duration (in ticks) |
|---------|---------|---------|---------|
|RECOGNIZING     |welcome         |17000000         |5000000         |
|RECOGNIZING     |welcome to         |17000000         |6400000         |
|RECOGNIZING     |welcome to applied math          |17000000         |13600000         |
|RECOGNIZING     |welcome to applied mathematics         |17000000         |17200000         |
|RECOGNIZING     |welcome to applied mathematics course         |17000000         |23700000         |
|RECOGNIZING     |welcome to applied mathematics course 2         |17000000         |26700000         |
|RECOGNIZING     |welcome to applied mathematics course 201         |17000000         |33400000         |
|RECOGNIZED     |Welcome to applied Mathematics course 201.         |17000000         |34500000         |

The total duration of the first utterance was 3.45 seconds. It was recognized at 1.7 to 5.15 seconds offset from the start of speech recognition (00:00:01.700 --> 00:00:05.150).

If the speaker continues to say "Let's get started," a new offset is calculated. Again, the offset is always calculated from the start of recognition to the start of an utterance. The following table shows potential offset and duration for an utterance that was recognized two seconds after the previous utterance ended.

|Event  |Text  |Offset (in ticks)  |Duration (in ticks) |
|---------|---------|---------|---------|
|RECOGNIZING     |OK         |71500000         |3100000         |
|RECOGNIZING     |OK now         |71500000         |10300000         |
|RECOGNIZING     |OK, now let's         |71500000         |14700000         |
|RECOGNIZING     |OK, now let's get started.         |71500000         |18500000         |
|RECOGNIZED     |OK, now let's get started.         |71500000         |20600000         |

The total duration of the second utterance was 2.06 seconds. It was recognized at 7.15 to 9.21 seconds offset from the start of speech recognition (00:00:07.150 --> 00:00:09.210) . 

Here is an example of how to use the result from a `Recognized` event to get the offset and duration.

```csharp
var startDateTime = new DateTime (e.Result.OffsetInTicks);
var endDateTime = startDateTime.Add (e.Result.Duration);
```

## Stable partial intermediate results

For captioning of prerecorded speech, typically you can wait for the final recognized transcription.  Then You will  before synchronizing it with the speech . 

Real time captioning presents unique challenges with respect latency versus accuracy. Events in real time are captured by the microphone and are processed by the speech recognition service.

The end of a single utterance is determined by listening for silence at the end or until a maximum of 15 seconds of audio is processed. The task returns the recognized text

You won't get the final recognition result until an utterance has completed. Recognizing events will provide the intermediate recognized text while an audio stream is being processed. Recognized events will provide the final recognized text once audio capture is completed

Recognizing events contain intermediate recognition results.
Recognized events contain final recognition results, which indicate a successful recognition attempt.

For example, if a speaker says "Welcome to applied Mathematics course 201."

```console
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied math
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZING: Text=welcome to applied mathematics course 2
RECOGNIZING: Text=welcome to applied mathematics course 201
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```

Consider how often you want to show captions, and whether you can wait for the final recognized text. To show captions as soon as possible, even if recognition is not yet completed, you can use intermediate text with the `Recognizing` event. If you want to show captions only when recognition is complete, wait for the final text with the `Recognized` event.

Stable partials will lead to less "flickering" but perhaps have more delay in showing the correct result. Supporting punctuation on intermediate results is not supported. 

Setting the stable partial result threshold to 0 will return all partial results.


```csharp
// The number of times a word has to be in partial results to be returned. 
speech_config.SetProperty (PropertyId.SpeechServiceResponse_StablePartialResultThreshold, 5);
```


Setting stable partial result threshold to 2:

```console
RECOGNIZING: Text=welcome
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZING: Text=welcome to applied mathematics course
RECOGNIZING: Text=welcome to applied mathematics course 2
RECOGNIZING: Text=welcome to applied mathematics course 20
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```

Setting stable partial result threshold to 5:

```console
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```


## Profanity filter 

Removes profanity (swearing), or replaces letters of profane words with stars
Masked: Replaces letters in profane words with star characters.
Removed: Removes profane words.

This enables profanity filter:
```csharp
speech_config.SetProfanity (ProfanityOption.Removed);
```

## Capitalize intermediate results
For more readable result, capitalization can help. 

Capitalizes intermediates? But does not add punctuation? 
Semi-display processing?
Lexical form is just the letters in a transcript (e.g., six - not '6'), no caps, etc

```csharp
// A string value specifying which post processing option should be used by service. Allowed value: TrueText
speech_config.SetProperty ("SpeechServiceResponse_PostProcessingOption", "TrueText");
```

## Improve recognition accuracy

For specific terms/names (e.g. captioning a sports game), Phrase list would help.

For general topics (e.g. captioning of orthodontist lectures), customization would help. Hereâ€™s how you can create a custom model



## Language identification
Multi-lingual
Short switches not supported well (in the LID doc already)



## Next steps
* [Get started with speech to text](get-started-speech-to-text.md)
