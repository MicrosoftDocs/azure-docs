---
title: 'Quickstart: Recognize speech in Objective-C on iOS using the Cognitive Services Speech SDK'
titleSuffix: "Microsoft Cognitive Services"
description: Learn how to recognize speech in Objective-C on iOS using the Cognitive Services Speech SDK
services: cognitive-services
author: chlandsi

ms.service: cognitive-services
ms.technology: Speech
ms.topic: article
ms.date: 09/24/2018
ms.author: chlandsi
---

# Quickstart: Recognize speech in Objective-C on iOS using the Cognitive Services Speech SDK

[!INCLUDE [Selector](../../../includes/cognitive-services-speech-service-quickstart-selector.md)]

In this article, you learn how to create an iOS app in Objective-C using the Cognitive Services Speech SDK to transcribe an audio file with recorded speech to text.

## Prerequisites

* A subscription key for the Speech service. See [Try the speech service for free](get-started.md).
* A Mac with XCode 9.4.1 installed as iOS development environment. This tutorial targets iOS versions 10 or later. If you don't have XCode yet, you can install it from the [App Store](https://geo.itunes.apple.com/us/app/xcode/id497799835?mt=12).

## Get the Speech SDK for iOS

[!INCLUDE [License Notice](../../../includes/cognitive-services-speech-service-license-notice.md)]

The current version of the Cognitive Services Speech SDK is `1.0.0`.

The Cognitive Services Speech SDK for Mac and iOS can be downloaded as a zip-file from https://aka.ms/csspeech/iosbinary. Download and copy the files to the `speechsdk` subdirectory in your home directory.


## Create an XCode Project 

Start XCode, and start a new project by clicking **File** > **New** > **Project**.
In the template selection dialog, choose the "iOS Single View App" template.

In the dialogs that follow, make the following selections:

1. App Info Screen
    1. Enter a name for the quickstart app, for example `helloworld`.
	1. Enter an organization name such as `TestOrg`.
	1. Disable all tests for this example project.
1. Select project directory
	1. Choose your home directory to put the project in. This will create a `helloworld` directory in your home directory that contains all the files for the XCode project.
	1. Disable the creation of a Git repo for this example project.
1. Adjust the paths to the SDK in the *Project Settings*
	1. On the **General** tab, add the SDK library as a framework: **Add framework** > **Add other...** > *Navigate to the directory containing the library*.
	1. On the **Build Settings** tab, add the path to SDK headers to the *header search path* (`$(SRCROOT)/../speechsdk/include`).
	1. Also in the **Build Settings** tab, add the path to the directory containing the library to the *library search path* (`$(SRCROOT)/../speechsdk/lib`).
	1. Still in the **Build Settings** tab, add the path to the directory containing the library to the *Runpath search path* (`$(SRCROOT)/../speechsdk/lib`).

TODO: screenshot with the relevant settings

## Set up the UI

The example app will have a very simple UI: A button to start the processing of the file, and a text label to display the result.
The UI is set up in the `Main.storyboard` part of the project.
Open the XML view of the storyboard by right-clicking the `Main.storyboard` entry of the project tree and selecting **Open As...** > **Source Code**.
Replace the autogenerated XML with this:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<document type="com.apple.InterfaceBuilder3.CocoaTouch.Storyboard.XIB" version="3.0" toolsVersion="14113" targetRuntime="iOS.CocoaTouch" propertyAccessControl="none" useAutolayout="YES" useTraitCollections="YES" useSafeAreas="YES" colorMatched="YES" initialViewController="BYZ-38-t0r">
    <device id="retina4_7" orientation="portrait">
        <adaptation id="fullscreen"/>
    </device>
    <dependencies>
        <deployment identifier="iOS"/>
        <plugIn identifier="com.apple.InterfaceBuilder.IBCocoaTouchPlugin" version="14088"/>
        <capability name="Safe area layout guides" minToolsVersion="9.0"/>
        <capability name="documents saved in the Xcode 8 format" minToolsVersion="8.0"/>
    </dependencies>
    <scenes>
        <!--Start Recognition View Controller-->
        <scene sceneID="tne-QT-ifu">
            <objects>
                <viewController id="BYZ-38-t0r" customClass="ViewController" sceneMemberID="viewController">
                    <view key="view" contentMode="scaleToFill" id="8bC-Xf-vdC">
                        <rect key="frame" x="0.0" y="0.0" width="375" height="667"/>
                        <autoresizingMask key="autoresizingMask" widthSizable="YES" heightSizable="YES"/>
                        <subviews>
                            <button opaque="NO" contentMode="scaleToFill" fixedFrame="YES" contentHorizontalAlignment="center" contentVerticalAlignment="center" buttonType="roundedRect" lineBreakMode="middleTruncation" translatesAutoresizingMaskIntoConstraints="NO" id="qFP-u7-47Q">
                                <rect key="frame" x="110" y="250" width="176" height="82"/>
                                <autoresizingMask key="autoresizingMask" flexibleMaxX="YES" flexibleMaxY="YES"/>
                                <state key="normal" title="Recognize!"/>
                                <connections>
                                    <action selector="recognizeButtonTapped:" destination="BYZ-38-t0r" eventType="touchUpInside" id="Vfr-ah-nbC"/>
                                </connections>
                            </button>
                            <label opaque="NO" userInteractionEnabled="NO" contentMode="center" horizontalHuggingPriority="251" verticalHuggingPriority="251" fixedFrame="YES" text="Recognition result" textAlignment="center" lineBreakMode="tailTruncation" numberOfLines="5" baselineAdjustment="alignBaselines" adjustsFontSizeToFit="NO" translatesAutoresizingMaskIntoConstraints="NO" id="tq3-GD-ljB">
                                <rect key="frame" x="24" y="402" width="335" height="148"/>
                                <autoresizingMask key="autoresizingMask" flexibleMaxX="YES" flexibleMaxY="YES"/>
                                <fontDescription key="fontDescription" type="system" pointSize="17"/>
                                <nil key="textColor"/>
                                <nil key="highlightedColor"/>
                            </label>
                        </subviews>
                        <color key="backgroundColor" red="1" green="1" blue="1" alpha="1" colorSpace="custom" customColorSpace="sRGB"/>
                        <viewLayoutGuide key="safeArea" id="6Tk-OE-BBY"/>
                    </view>
                    <connections>
                        <outlet property="recognitionResultLabel" destination="tq3-GD-ljB" id="kP4-o4-s0Q"/>
                        <outlet property="recognizeButton" destination="qFP-u7-47Q" id="dqX-Pp-pCL"/>
                    </connections>
                </viewController>
                <placeholder placeholderIdentifier="IBFirstResponder" id="dkx-z0-nzr" sceneMemberID="firstResponder"/>
            </objects>
            <point key="canvasLocation" x="135.19999999999999" y="132.68365817091455"/>
        </scene>
    </scenes>
</document>
```

## Add the sample code

1. Download the [sample wav file](https://raw.githubusercontent.com/Azure-Samples/Cognitive-Speech-STT-Android/95b698b584bce01d7cfa7faa15fb254482b6402e/samples/SpeechRecoExample/assets/whatstheweatherlike.wav) by right-clicking it and choosing **Save target as...**.
Add the wav file to the project as a resource by dragging it from a Finder window into the root level of the Project view.
1. Replace the contents of the autogenerated `ViewController.m` file by:
    ```objectivec
    #import "ViewController.h"
    #import "speech_factory.h"

    @interface ViewController ()
    @property (weak, nonatomic) IBOutlet UIButton *recognizeButton;
    @property (weak, nonatomic) IBOutlet UILabel *recognitionResultLabel;
    - (IBAction)recognizeButtonTapped:(UIButton *)sender;
    @end

    @implementation ViewController

    - (void)viewDidLoad {
        [super viewDidLoad];
        // Do any additional setup after loading the view.
    }

    - (void)didReceiveMemoryWarning {
        [super didReceiveMemoryWarning];
        // Dispose of any resources that can be recreated.
    }

    - (IBAction)recognizeButtonTapped:(UIButton *)sender {
        __block bool end = false;
        SpeechFactory *factory = [SpeechFactory fromSubscription:@"YourSubscriptionKey" AndRegion:@"YourServiceRegion"];

        NSBundle *mainBundle = [NSBundle mainBundle];
        NSString *myFile = [mainBundle pathForResource: @"whatstheweatherlike" ofType: @"wav"];
        NSLog(@"Main bundle path: %@", mainBundle);
        NSLog(@"myFile path: %@", myFile);

        NSLog(@"Factory subscription key %@.",factory.subscriptionKey);
        SpeechRecognizer *recognizer = [factory createSpeechRecognizerWithFileInput:myFile];

        [recognizer addFinalResultEventListener: ^ (SpeechRecognizer * recognizer, SpeechRecognitionResultEventArgs *eventArgs) {
            NSLog(@"Received final result event. SessionId: %@, recognition result:%@. Status %ld.", eventArgs.sessionId, eventArgs.result.text, (long)eventArgs.result.recognitionStatus);
            dispatch_async(dispatch_get_main_queue(), ^{
                [self updateRecognitionResultText:(eventArgs.result.text)];
            });
            end = true;
        }];

        [recognizer startContinuousRecognition];
        while (end == false)
            [NSThread sleepForTimeInterval:1.0f];
        [recognizer stopContinuousRecognition];

        [recognizer close];
    }

    - (void)updateRecognitionResultText:(NSString *) resultText {
        self.recognitionResultLabel.text = resultText;
    }

    @end
    ```
    <!-- [!code-cpp[Quickstart Code](~/samples-cognitive-services-speech-sdk/quickstart/objectivec-ios/helloworld/helloworld/ViewController.m#code)] -->
1. Replace the string `YourSubscriptionKey` with your subscription key.

1. Replace the string `YourServiceRegion` with the [region](regions.md) associated with your subscription (for example, `westus` for the free trial subscription).


TODO: screenshot of resulting project tree

## Building and Running the Sample

1. Make the debug output visible (**View** > **Debug Area** > **Activate Console**).
1. Build and run the example code in the iOS simulator by selecting **Product** -> **Run** from the menu or clicking the **Play** button.
1. After you click the "Recognize!" button in the app, you should see the contents of the audio file "What's the weather like?" on the lower part of the simulated screen.

 ![Simulated iOS App](media/sdk/qs-objectivec-simulated-app.png)

[!INCLUDE [Download the sample](../../../includes/cognitive-services-speech-service-speech-sdk-sample-download-h2.md)]
Look for this sample in the `quickstart/objectivec-ios` folder.

## Next steps

* [Get our samples](speech-sdk.md#get-the-samples)
