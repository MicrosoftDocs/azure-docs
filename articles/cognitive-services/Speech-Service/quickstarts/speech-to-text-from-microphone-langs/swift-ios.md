---
title: 'Quickstart: Recognize speech from a microphone, Swift - Speech service'
titleSuffix: Azure Cognitive Services
description: Learn how to recognize speech in Swift on iOS by using the Speech SDK
services: cognitive-services
author: chlandsi
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: quickstart
ms.date: 06/25/2020
ms.author: chlandsi
---

# Quickstart: Recognize speech in Swift on iOS by using the Speech SDK

Quickstarts are also available for [speech synthesis](~/articles/cognitive-services/Speech-Service/quickstarts/text-to-speech-langs/swift-ios.md).

In this article, you learn how to create an iOS app in Swift by using the Azure Cognitive Services Speech SDK to transcribe speech recorded from a microphone to text.

## Prerequisites

Before you get started, you'll need:

* A [subscription key](~/articles/cognitive-services/Speech-Service/get-started.md) for the Speech service.
* A macOS machine with [Xcode 9.4.1](https://geo.itunes.apple.com/us/app/xcode/id497799835?mt=12) or later and [CocoaPods](https://cocoapods.org/) installed.

## Get the Speech SDK for iOS

[!INCLUDE [License notice](~/includes/cognitive-services-speech-service-license-notice.md)]

This tutorial won't work with a version of the SDK earlier than 1.6.0.

The Cognitive Services Speech SDK for iOS is distributed as a framework bundle. It can be used in Xcode projects as a [CocoaPod](https://cocoapods.org/) or downloaded from https://aka.ms/csspeech/iosbinary and linked manually. This article uses a CocoaPod.

## Create an Xcode project

Start Xcode, and start a new project by selecting **File** > **New** > **Project**.
In the template selection dialog box, select the **iOS Single View App** template.

In the dialog boxes that follow, make the following selections.

1. In the **Project Options** dialog box:
    1. Enter a name for the quickstart app, for example, *helloworld*.
    1. Enter an appropriate organization name and an organization identifier if you already have an Apple developer account. For testing purposes, use a name like *testorg*. To sign the app, you need a proper provisioning profile. For more information, see the [Apple developer site](https://developer.apple.com/).
    1. Make sure **Swift** is chosen as the language for the project.
    1. Disable the check boxes to use storyboards and to create a document-based application. The simple UI for the sample app is created programmatically.
    1. Clear all the check boxes for tests and core data.
1. Select a project directory:
    1. Choose a directory to put the project in. This step creates a helloworld directory in the chosen directory that contains all the files for the Xcode project.
    1. Disable the creation of a Git repo for this example project.
1. The app also needs to declare use of the microphone in the `Info.plist` file. Select the file in the overview, and add the **Privacy - Microphone Usage Description** key with a value like *Microphone is needed for speech recognition*.

    ![Settings in Info.plist](~/articles/cognitive-services/Speech-Service/media/sdk/qs-swift-ios-info-plist.png)

1. Close the Xcode project. You use a different instance of it later after you set up the CocoaPods.

## Add the sample code

1. Place a new header file with the name `MicrosoftCognitiveServicesSpeech-Bridging-Header.h` into the helloworld directory inside the helloworld project. Paste the following code into it:

   [!code-objectivec[Quickstart code](~/samples-cognitive-services-speech-sdk/quickstart/swift/ios/from-microphone/helloworld/helloworld/MicrosoftCognitiveServicesSpeech-Bridging-Header.h#code)]

1. Add the relative path `helloworld/MicrosoftCognitiveServicesSpeech-Bridging-Header.h` to the bridging header to the Swift project settings for the helloworld target in the **Objective-C Bridging Header** field.

   ![Header properties](~/articles/cognitive-services/Speech-Service/media/sdk/qs-swift-ios-bridging-header.png)

1. Replace the contents of the autogenerated `AppDelegate.swift` file with the following code:

   [!code-swift[Quickstart code](~/samples-cognitive-services-speech-sdk/quickstart/swift/ios/from-microphone/helloworld/helloworld/AppDelegate.swift#code)]
1. Replace the contents of the autogenerated `ViewController.swift` file with the following code:

   [!code-swift[Quickstart code](~/samples-cognitive-services-speech-sdk/quickstart/swift/ios/from-microphone/helloworld/helloworld/ViewController.swift#code)]
1. In `ViewController.swift`, replace the string `YourSubscriptionKey` with your subscription key.
1. Replace the string `YourServiceRegion` with the [region](~/articles/cognitive-services/Speech-Service/regions.md) associated with your subscription. For example, use `westus` for the free trial subscription.

## Install the SDK as a CocoaPod

1. Install the CocoaPod dependency manager as described in its [installation instructions](https://guides.cocoapods.org/using/getting-started.html).
1. Go to the directory of your sample app, which is helloworld. Place a text file with the name *Podfile* and the following content in that directory:

   [!code-ruby[Quickstart code](~/samples-cognitive-services-speech-sdk/quickstart/swift/ios/from-microphone/helloworld/Podfile)]
1. Go to the helloworld directory in a terminal, and run the command `pod install`. This command generates a `helloworld.xcworkspace` Xcode workspace that contains both the sample app and the Speech SDK as a dependency. This workspace is used in the following steps.

## Build and run the sample

1. Open the workspace `helloworld.xcworkspace` in Xcode.
1. Make the debug output visible by selecting **View** > **Debug Area** > **Activate Console**.
1. Choose either the iOS simulator or an iOS device connected to your development machine as the destination for the app from the list in the **Product** > **Destination** menu.
1. Build and run the example code in the iOS simulator by selecting **Product** > **Run** from the menu. You also can select the **Play** button.
1. After you select the **Recognize** button in the app and say a few words, you should see the text you have spoken on the lower part of the screen.

## Next steps

> [!div class="nextstepaction"]
> [Explore our samples on GitHub](https://aka.ms/csspeech/samples)
