---
title: Create a Tenant Language Model - Speech Service
titleSuffix: Azure Cognitive Services
description: Automatically generate a custom speech model that leverages your Office365 data to deliver optimal speech recognition for organization-specific terms that is both secure and compliant.
services: cognitive-services
author: erhopf
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: tutorial
ms.date: 11/4/2019
ms.author: erhopf
---

# Create a Tenant Language Model (Preview)

Tenant Language Model is an opt-in service for Office365 enterprise customers that automatically generates a custom speech recognition model from your Office365 data. The model that's created is optimized for organization-specific terms, like technical terms and people's names, all in a secure and compliant way.

> [!IMPORTANT]
> If your organization enrolls with Tenant Language Model, the Speech service may access your organization’s language model, which is generated by Office 365 resources, such as emails and documents. Your organization’s Office 365 administrator may turn on/off the usage of the organization-wide language model using the Office 365 Admin Portal.

In this tutorial, you'll learn how to:

> [!div class="checklist"]
> * Enroll to use a Tenant Language Model in the Microsoft 365 Admin Center
> * Create a Tenant Language Model
> * Publish a Tenant Language Model
> * Use a Tenant Language Model with the Speech SDK
> * Update a Tenant Language Model

![Tenant Language Model diagram](tenant-language-model/tenant-laguage-model-diagram.png)

## Enroll using the Microsoft 365 Admin Center

Before you can create or publish a Tenant Language Model, you first need to enroll using the Microsoft 365 Admin Center. This task can only be completed by your Microsoft 365 Admin.

1. Sign into the [Microsoft 365 Admin Center](https://admin.microsoft.com ).
2. From the left panel, select **Settings** then **Apps**.
   ![Tenant Language Model diagram](tenant-language-model/tenant-laguage-model-enrollment.png)
3. Locate and select **Azure Speech Services**.
   ![Tenant Language Model diagram](tenant-language-model/tenant-laguage-model-enrollment-2.png)
4. Click the checkbox and save.

If you need to turn off the Tenant Language Model, navigate back to this screen, deselect the checkbox and save.

## Create a model

After your admin has enabled Tenant Language Model for your organization, you can create a language model based on your Office365 data.

1. Sign into the [Speech Studio](https://speech.microsoft.com/).
2. In the upper right corner, locate and click the gear icon (settings), then select **Tenant Model setting**.
3. At this point you'll see a message letting you know if you are qualified to create a Tenant Language Model.
   > [!NOTE]
   > Office 365 enterprise customers in NAM, EURO, and APAC are eligible to create a Tenant Language Model (English). If you are a Customer Lockbox (CLB) or Customer Key (CK) customer, this feature isn't available. To determine if you are a Customer Lockbox or Customer Key customer, follow these instructions:
   > * [Customer Lockbox](https://docs.microsoft.com/office365/securitycompliance/controlling-your-data-using-customer-key#FastTrack)
   > * [Customer Key](https://docs.microsoft.com/en-us/microsoft-365/compliance/customer-lockbox-requests)

4. Next, select **Train**. You'll receive an email with instructions when your Tenant Language Model is ready.

## Publish your model

When your Tenant Language Model is ready, there are two ways to publish it.

* **Option 1:** Click the **Publish** button in the email that you received.
* **Option 2:** Publish via Speech Studio
  1. Sign into the [Speech Studio](https://speech.microsoft.com/).
  2. In the upper right corner, locate and click the gear icon (settings), then select **Tenant Model setting**.
  3. Click **Publish**.
  4. When your model is ready, the status will change to **Succeeded**.

## Use your model with the Speech SDK

Now that you've published your model, you can use it with the Speech SDK. In this section, you'll create an **App registration** for use with **Azure Active Directory (AAD)**. Then using the sample code provided you'll call the Speech Service using AAD authentication.

Let's start by registering your app with AAD:

1. Sign into the [Azure portal](https://ms.portal.azure.com/).
2. From the left navigation, select **Azure Active Directory**, then select **App registration**.
3. Click **+New application** to register your app.
4. Get the **Application ID**. You'll use this with the sample code.  

> [!NOTE]
> For detailed AAD instructions, see [Register an application with the Microsoft identity platform](https://docs.microsoft.com/azure/active-directory/develop/quickstart-register-app).

Next, let's look at the code you'll use to call the Speech SDK in C# with a Tenant Language Model. This guide presumes that your platform is already setup. If you need help setting up, see [Platform setup: C#](#placeholder).

Copy this code into your project:

```csharp
namespace PrincetonSROnly.FrontEnd.Samples
{
    using System;
    using System.Collections.Generic;
    using System.IO;
    using System.Net.Http;
    using System.Text;
    using System.Text.RegularExpressions;
    using System.Threading.Tasks;
    using Microsoft.CognitiveServices.Speech;
    using Microsoft.CognitiveServices.Speech.Audio;
    using Microsoft.IdentityModel.Clients.ActiveDirectory;
    using Newtonsoft.Json.Linq;

    // Note: ServiceApplicationId is a fixed value.  No need to change.

    public class TenantLMSample
    {
        private const string EndpointUri = "EndpointUri";
        private const string SubscriptionKey = "SubscriptionKey";
        private const string Username = "Username";
        private const string Password = "Password";
        private const string ClientApplicationId = "Your-Client-App-ID";
        private const string ServiceApplicationId = "18301695-f99d-4cae-9618-6901d4bdc7be";

        public static async Task ContinuousRecognitionWithTenantLMAsync(Uri endpointUri, string subscriptionKey, string audioDirPath, string username, string password)
        {
            var config = SpeechConfig.FromEndpoint(endpointUri, subscriptionKey);

            // Passing client specific information for obtaining LM
            if (string.IsNullOrEmpty(username) || string.IsNullOrEmpty(password))
            {
                config.AuthorizationToken = await AcquireAuthTokenWithInteractiveLoginAsync().ConfigureAwait(false);
            }
            else
            {
                config.AuthorizationToken = await AcquireAuthTokenWithUsernamePasswordAsync(username, password).ConfigureAwait(false);
            }

            var stopRecognition = new TaskCompletionSource<int>();

            // Creates a speech recognizer using file as audio input.
            // Replace with your own audio file name.
            using (var audioInput = AudioConfig.FromWavFileInput(audioDirPath))
            {
                using (var recognizer = new SpeechRecognizer(config, audioInput))
                {
                    // Subscribes to events
                    recognizer.Recognizing += (s, e) =>
                    {
                        Console.WriteLine($"RECOGNIZING: Text={e.Result.Text}");
                    };

                    recognizer.Recognized += (s, e) =>
                    {
                        if (e.Result.Reason == ResultReason.RecognizedSpeech)
                        {
                            Console.WriteLine($"RECOGNIZED: Text={e.Result.Text}");
                        }
                        else if (e.Result.Reason == ResultReason.NoMatch)
                        {
                            Console.WriteLine($"NOMATCH: Speech could not be recognized.");
                        }
                    };

                    recognizer.Canceled += (s, e) =>
                    {
                        Console.WriteLine($"CANCELED: Reason={e.Reason}");
                        if (e.Reason == CancellationReason.Error)
                        {
                            Exception exp = new Exception(string.Format("Error Code: {0}\nError Details{1}\nIs your subscription information updated?", e.ErrorCode, e.ErrorDetails));
                            throw exp;
                        }

                        stopRecognition.TrySetResult(0);
                    };

                    recognizer.SessionStarted += (s, e) =>
                    {
                        Console.WriteLine("\n    Session started event.");
                    };

                    recognizer.SessionStopped += (s, e) =>
                    {
                        Console.WriteLine("\n    Session stopped event.");
                        Console.WriteLine("\nStop recognition.");
                        stopRecognition.TrySetResult(0);
                    };

                    // Starts continuous recognition. Uses StopContinuousRecognitionAsync() to stop recognition.
                    await recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);

                    // Waits for completion.
                    // Use Task.WaitAny to keep the task rooted.
                    Task.WaitAny(new[] { stopRecognition.Task });

                    // Stops recognition.
                    await recognizer.StopContinuousRecognitionAsync().ConfigureAwait(false);
                }
            }
        }

        public static void Main(string[] args)
        {
            var arguments = new Dictionary<string, string>();
            string inputArgNamePattern = "--";
            Regex regex = new Regex(inputArgNamePattern);
            if (args.Length > 0)
            {
                foreach (var arg in args)
                {
                    var userArgs = arg.Split("=");
                    arguments[regex.Replace(userArgs[0], string.Empty)] = userArgs[1];
                }
            }

            var endpointString = arguments.GetValueOrDefault(EndpointUri, $"wss://westus.online.princeton.customspeech.ai/msgraphcustomspeech/conversation/v1");
            var endpointUri = new Uri(endpointString);

            if (!arguments.ContainsKey(SubscriptionKey))
            {
                Exception exp = new Exception("Subscription Key missing! Please pass in a Cognitive services subscription Key using --SubscriptionKey=\"your_subscription_key\"" +
                    "Find more information on creating a Cognitive services resource and accessing your Subscription key here: https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows");
                throw exp;
            }

            var subscriptionKey = arguments[SubscriptionKey];
            var username = arguments.GetValueOrDefault(Username, null);
            var password = arguments.GetValueOrDefault(Password, null);

            var audioDirPath = Path.Combine(Path.GetDirectoryName(System.Reflection.Assembly.GetExecutingAssembly().Location), "../../../AudioSamples/DictationBatman.wav");
            if (!File.Exists(audioDirPath))
            {
                Exception exp = new Exception(string.Format("Audio File does not exist at path: {0}", audioDirPath));
                throw exp;
            }

            ContinuousRecognitionWithTenantLMAsync(endpointUri, subscriptionKey, audioDirPath, username, password).GetAwaiter().GetResult();
        }

        private static async Task<string> AcquireAuthTokenWithUsernamePasswordAsync(string username, string password)
        {
            var tokenEndpoint = "https://login.microsoftonline.com/common/oauth2/token";
            var postBody = $"resource={ServiceApplicationId}&client_id={ClientApplicationId}&grant_type=password&username={username}&password={password}";
            var stringContent = new StringContent(postBody, Encoding.UTF8, "application/x-www-form-urlencoded");
            using (HttpClient httpClient = new HttpClient())
            {
                var response = await httpClient.PostAsync(tokenEndpoint, stringContent).ConfigureAwait(false);

                if (response.IsSuccessStatusCode)
                {
                    var result = await response.Content.ReadAsStringAsync().ConfigureAwait(false);

                    JObject jobject = JObject.Parse(result);
                    return jobject["access_token"].Value<string>();
                }
                else
                {
                    throw new Exception($"Requesting token from {tokenEndpoint} failed with status code {response.StatusCode}: {await response.Content.ReadAsStringAsync().ConfigureAwait(false)}");
                }
            }
        }

        private static async Task<string> AcquireAuthTokenWithInteractiveLoginAsync()
        {
            var authContext = new AuthenticationContext("https://login.windows.net/microsoft.onmicrosoft.com");
            var deviceCodeResult = await authContext.AcquireDeviceCodeAsync(ServiceApplicationId, ClientApplicationId).ConfigureAwait(false);

            Console.WriteLine(deviceCodeResult.Message);

            var authResult = await authContext.AcquireTokenByDeviceCodeAsync(deviceCodeResult).ConfigureAwait(false);

            return authResult.AccessToken;
        }
    }
}
```

There are a few parameters you'll need to update in the sample before you can run it:

1. Add either (1) `Username` and `Password`, or (2) `ClientApplicationId`. You don't need to provide both. The `ClientApplicationId` is the value returned in step 4 of your AAD app registration.
2. Replace the value of `SubscriptionKey` with the subscription key for your Speech resource in the Azure portal.
3. Update the `EndpointUri`. Make sure that you replace `{your-region}` with the region where your Speech resource was created. If you don't remember, you can find this information in the overview blade for your resource in the Azure portal. Keep in mind, only `westus` and `eastus` are currently supported.
   ```csharp
   $"wss://{your region}.online.princeton.customspeech.ai/msgraphcustomspeech/conversation/v1".
   ```

## Update your model

While Tenant Language Model is in preview, you'll be able to update the language model manually once per year. When you update the language model, the original is replaced.

1. Sign into the [Speech Studio](https://speech.microsoft.com/).
2. In the upper right corner, locate and click the gear icon (settings), then select **Tenant Model setting**.
3. Locate and select **Update**.
4. You'll receive an email with instructions when your Tenant Language Model is ready.
5. When the model is ready, [publish your model](#publish-your-model).

## See also

* [Speech Studio](https://speech.microsoft.com/)
* [Speech SDK](speech-sdk.md)
