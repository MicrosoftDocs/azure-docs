<properties 
	pageTitle="Manipulate encoding tasks by customizing task presets" 
	description="The Azure Media Services Encoder allows you to pass custom preset files to Azure Media Encoder. This topic shows how to customize preset files in order to achieve the following tasks: overlay an image onto an existing video, control the output file names that the encoder produces, stitch videos. " 
	services="media-services" 
	documentationCenter="" 
	authors="juliako" 
	manager="dwrede" 
	editor=""/>

<tags 
	ms.service="media-services" 
	ms.workload="media" 
	ms.tgt_pltfrm="na" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.date="09/07/2015" 
	ms.author="juliako"/>

#Manipulate encoding tasks by customizing task presets 

The Azure Media Services Encoder allows you to pass custom preset files to Azure Media Encoder. This topic shows how to customize preset files in order to achieve the following tasks: 

- overlay an image onto an existing video, 
- control the output file names that the encoder produces, 
- stitch videos,
- encode presentations with mostly speech.

##Controlling Azure Media Encoder Output File Names 

By default, the Azure Media Encoder creates output filenames by combining various attributes of the input asset and the encoding process. Each attribute is identified using a macro as discussed below.

The following is a complete list of the macros available for output file naming:
Audio Bitrate - the bitrate used when encoding the audio, specified in kbps

- Audio Codec the codec used for encoding audio, valid values are: AAC, WMA, and DDP
- Channel Count the number of audio channels encoded, valid values are: 1, 2, or 6
- Default extension – the default file extension 
- Language the BCP-47 language code representing the language used in the audio. This currently defaults to “und”. 
- Original File Name the name of the file uploaded into Azure Storage
- StreamId – the stream ID as defined by the streamID attribute of the <StreamInfo> element in the preset file 
- Video Codec the codec used for encoding, valid values are: H264 and VC1
- Video Bitrate the bitrate used when encoding the video, specified in kbps

These macros can be combine in any permutation to control the name of the files generated by the Media Services Encoder. For example, the default naming convention is:

	{Original File Name}_{Video Codec}{Video Bitrate}{Audio Codec}{Language}{Channel Count}{Audio Bitrate}.{Default Extension}

The file naming convention is specified using the DefaultMediaOutputFileName attribute of the [Preset](https://msdn.microsoft.com/library/azure/dn554334.aspx) element. For example:

	<Preset DefaultMediaOutputFileName="{Original file name}{StreamId}_LongOutputFileName{Bit Rate}{Video Codec}{Video Bitrate}{Audio Codec}{Audio Bitrate}{Language}{Channel Count}.{Default extension}"
	  Version="5.0">
	<MediaFile …>
	   <OutputFormat>
	      <MP4OutputFormat StreamCompatibility="Standard">
	         <VideoProfile>
	            <MainH264VideoProfile … >
	               <Streams  AutoSize="False"
	                         FreezeSort="False">
	                  <StreamInfo StreamId="1"
	                              Size="1280, 720">
	                     <Bitrate>
	                       <ConstantBitrate Bitrate="3400"
	                                        IsTwoPass="False"
	                                        BufferWindow="00:00:05" />
	                     </Bitrate>
	                   </StreamInfo>
	                  </Streams>
	               </MainH264VideoProfile>
	            </VideoProfile>
	         </MP4OutputFormat>
	   </OutputFormat>
	</MediaFile>

The encoder will insert underscores between each macro, for example, configuration above would result in a file name like: MyVideo_H264_4500kpbs_AAC_und_ch2_128kbps.mp4.


##Creating Overlays

The Azure Media Services Encoder allows you to overlay an image (jpg, bmp, gif, tif), a video, or an audio track (*.wma, *.mp3, *.wav) onto an existing video. This functionality is similar to that of Expression Encoder 4 (Service Pack 2).

###Overlays with the Media Services Encoder

You can specify when the overlay will be presented, the duration the overlay will presented, and for image/video overlays where on the screen the overlay will appear. You can also have the overlays fade in and/or fade out. The audio/video files to overlay can be contained in multiple assets or a single asset. Overlays are controlled by the preset XML that is passed to the encoder. For a complete description of the preset schema, see Azure Media Encoder Schemas. Overlays are specified in the <MediaFile> element as shown in the following preset snippet:

	<MediaFile
	    ...
	    OverlayFileName="%1%"
	    OverlayRect="257, 144, 255, 144"
	    OverlayOpacity="0.9"
	    OverlayFadeInDuration="00:00:02"
	    OverlayFadeOutDuration="00:00:02"
	    OverlayLayoutMode="Custom"
	    OverlayStartTime="00:00:05"
	    OverlayEndTime="00:00:10.2120000"
	
	    AudioOverlayFileName="%2%"
	    AudioOverlayLoop="True"
	    AudioOverlayLoopingGap="00:00:00"
	    AudioOverlayLayoutMode="WholeSequence"
	    AudioOverlayGainLevel="2.2"
	    AudioOverlayFadeInDuration="00:00:00"
	    AudioOverlayFadeOutDuration="00:00:00">
	    ...
	</MediaFile>

###Presets for Video or Image Overlays

Overlays can be from a single or multiple assets. When creating video overlays using multiple assets, the overlay filename is specified in the OverlayFileName attribute using %n% syntax where n is the zero-based index of the input assets for the encoding task. When creating video overlays with a single asset, the overlay file name is specified directly into the OverlayFileName attribute, as shown in the following preset snippets:

Example 1:

	<!-- Multiple Assets -->
	<MediaFile
		...
		OverlayFileName="%1%"
		OverlayRect="257, 144, 255, 144"
		OverlayOpacity="0.9"
		OverlayFadeInDuration="00:00:02"
		OverlayFadeOutDuration="00:00:02"
		OverlayLayoutMode="Custom"
		OverlayStartTime="00:00:05"
		OverlayEndTime="00:00:10.2120000">

Example 2:

	<!-- Single Asset -->
	<MediaFile
		...
		OverlayFileName="videoOverlay.mp4"
		OverlayRect="257, 144, 255, 144"
		OverlayOpacity="0.9"
		OverlayFadeInDuration="00:00:02"
		OverlayFadeOutDuration="00:00:02"
		OverlayLayoutMode="Custom"
		OverlayStartTime="00:00:05"
		OverlayEndTime="00:00:10.2120000">

The location and size of the video overlay is controlled by the OverlayRect attribute. The content that is to be overlaid will be re-sized to fit this rectangle. Opacity is controlled by the OverlayOpacity attribute. Valid values are 0.0 – 1.0, where 1.0 is 100% opaque. The overlay will be displayed at the time specified by the OverlayStartTime attribute and will end at the time specified by the OverlayEndTime attribute. Both OverlayStartTime and OverlayEndTime should fall within the timeline of the source video. For more information about each overlay-specific attribute, please see Azure Media Encoder Schemas.

###Presets for Audio Overlays

Audio overlays can be any supported audio file format, for example. For a complete list of supported audio file formats, see Formats Supported by the Media Services Encoder. Audio overlays are also specified in the <MediaFile> element as shown in the following preset snippet:

	<MediaFile
		...
		AudioOverlayFileName="%1%" <!-- or AudioOverlayFileName=”audioOverlay.mp3” for overlays from a single asset -->
		AudioOverlayLoop="True"
		AudioOverlayLoopingGap="00:00:00"
		AudioOverlayLayoutMode="Custom"
		AudioOverlayStartTime="00:05:00"
		AudioOverlayEndTime="00:10:00"
		AudioOverlayGainLevel="2.2"
		AudioOverlayFadeInDuration="00:00:00"
		AudioOverlayFadeOutDuration="00:00:00">

For audio overlays stored in multiple assets, the audio overlay filename is specified in the AudioOverlayFileName attribute using %n% syntax, where n is the zero-based index of the collection of input assets to the encoding Task. For audio overlays stored in a single asset the overlay filename is specified directly in the AudioOverlayFileName attribute. The AudioOverlayLayoutMode determines when the audio overlay will be presented. When set to “WholeSequence” the audio track will be presented during the entire duration of the video. When set to “Custom” the AudioOverlayStartTime and AudioOverlayEndTime attributes determine when the audio overlay begins and ends. Both OverlayStartTime and OverlayEndTime should fall within the timeline of the source video. For more information on all of the audio overlay attributes, see the Azure Media Encoder Schemas. Audio overlays can be combined with video overlays as shown in the following preset snippet:
	
	<MediaFile
	    DeinterlaceMode="AutoPixelAdaptive"
	    ResizeQuality="Super"
	    AudioGainLevel="1"
	    VideoResizeMode="Stretch"
	    OverlayFileName="%1%"
	    OverlayRect="257, 144, 255, 144"
	    OverlayOpacity="0.9"
	    OverlayFadeInDuration="00:00:02"
	    OverlayFadeOutDuration="00:00:02"
	    OverlayLayoutMode="Custom"
	    OverlayStartTime="00:00:05"
	    OverlayEndTime="00:00:10.2120000"
	    AudioOverlayFileName="%2%"
	    AudioOverlayLoop="True"
	    AudioOverlayLoopingGap="00:00:00"
	    AudioOverlayLayoutMode="Custom"
	    AudioOverlayStartTime="00:05:00"
	    AudioOverlayEndTime="00:10:00"
	    AudioOverlayGainLevel="2.2"
	    AudioOverlayFadeInDuration="00:00:00"
	    AudioOverlayFadeOutDuration="00:00:00">


###Submitting Tasks with Overlays

Once you have created a preset file you must do the following:

- Upload your asset(s)
- Load the preset configuration (Note: the code below assumes the Preset above.
- Create a job
- Get a reference to the Media Services Encoder
- Create a task specifying the collection of input assets, the preset configuration, the media encoder, and the output asset
- Submit the job

The following code snippet shows how to do these steps:

	static public void CreateOverlayJob()
	{
        // Create and cache the Media Services credentials in a static class variable.
        _cachedCredentials = new MediaServicesCredentials(
                        MediaServicesAccountName,
                        MediaServicesAccountKey);
        // Used the cached credentials to create CloudMediaContext.
        _context = new CloudMediaContext(_cachedCredentials);


		// Upload assets to overlay
		IAsset inputAsset1 = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, video1.mp4); // this is the input mezzanine
		IAsset inputAsset2 = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, video2.wmv);// this will be used as a video overlay
		IAsset inputAsset3 = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, video3.wmv); // this will be used as an audio overlay
		
		// Load the preset configuration
		string presetFileName = "OverlayPreset.xml";
		string configuration = File.ReadAllText(presetFileName);
		
		// Create a job
		IJob job = _context.Jobs.Create("A AME overlay job, using " + presetFileName);
		         
		// Get a reference to the media services encoder   
		IMediaProcessor processor = GetLatestMediaProcessorByName("Azure Media Encoder");
		    
		// Create a task    
		ITask task = job.Tasks.AddNew("Encode Task for overlay, using " + presetFileName, processor, configuration, TaskOptions.None);
		
		// Add the input assets
		task.InputAssets.Add(inputAsset1);
		task.InputAssets.Add(inputAsset2);
		task.InputAssets.Add(inputAsset3);
		
		// Add the output asset
		task.OutputAssets.AddNew("Result of an overlay job, using " + presetFileName, AssetCreationOptions.None);
		
		// Submit the job
		job.Submit();
	}



>[AZURE.NOTE]This snippet loads each asset sequentially for simplicity. In production environments assets would be loaded in bulk. For more information on uploading multiple assets in bulk see [Ingesting Assets in Bulk with the Media Services SDK for .NET](media-services-dotnet-upload-files.md#ingest_in_bulk).

For complete sample code see [Creating Overlays with Media Services Encoder](https://code.msdn.microsoft.com/Creating-Audio-and-Video-c2942c47).  

###Error Conditions

When editing the Preset string, you must ensure the following:

- For video/image overlays, the overlay rectangle must fit entirely within the dimensions of the source video
- The start and end time of the overlays should be within the timeline of the source video
- If the preset XML contains a reference to ?OverlayFileName=”%n%”, then the InputAssets collection for the Tasks should contain at least n+1 Assets



##Stitching Video Segments

The media services encoder provides support for stitching together a set of videos. Videos can be stitched together end-to-end or you can specify portions of one or both videos to be stitched together. The result of the stitching is a single output asset that contains the specified video from the input assets. The videos to be stitched can be contained in multiple assets or a single asset. Stitching is controlled by the preset XML passed to the encoder. For a complete description of the preset schema, see [Azure Media Encoder Schema](https://msdn.microsoft.com/library/azure/dn584702.aspx). 

###Stitching with Media Services Encoder

Stitching is controlled within the <MediaFile> element as shown in the following preset:
	
	<MediaFile
	    DeinterlaceMode="AutoPixelAdaptive"
	    ResizeQuality="Super"
	    AudioGainLevel="1"
	    VideoResizeMode="Stretch">
	    <Sources>
	      <Source
	        AudioStreamIndex="0">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	      <Source
	        ResizeMode="Letterbox"
	        ApplyCrop="False"
	        AudioStreamIndex="0"
	        MediaFile="%1%">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	      <Source
	        ResizeMode="Letterbox"
	        ApplyCrop="False"
	        AudioStreamIndex="0"
	        MediaFile="%2%">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	     </Sources>
	</MediaFile>

For each video to be stitched, a <Source> element is added to the <Sources> element. Each <Source> element contains a <Clips> element. Each <Clips> element contains one or more <Clip> element that specifies how much of the video will be stitched into the output asset, by specifying a start and end time. The <Source> element references the asset on which it acts. The format of the reference depends on whether the videos to be stitched are in separate assets or in a single asset. If you want to stitch an entire video, simply omit the <Clips> element. 

###Stitching Videos from Multiple Assets

When stitching videos from multiple assets, a zero-based index is used for the MediaFile attribute of the <Source> element to identify which asset the <Source> element corresponds to. The zero index is not specified, the <Source> element that does not specify a MediaFile attribute references the first input asset. All other <Source> elements must specify the zero-based index of the input asset to which it refers by using %n% syntax where n is the zero-based index of the input asset. In the preceding example the first <Source> element specifies the first input asset, the second <Source> element specifies the second input asset, and so on. There is no requirement that the input assets be referenced in order, for example:
	
	<MediaFile
	    DeinterlaceMode="AutoPixelAdaptive"
	    ResizeQuality="Super"
	    AudioGainLevel="1"
	    VideoResizeMode="Stretch">
	    <Sources>
	      <Source
	        AudioStreamIndex="0"
	        MediaFile="%1%">
	        <Clips>
	          <Clip EndTime="00:00:05" 
	                StartTime="00:00:00" />
	        </Clips>
	                  
	        </Source>
	      <Source
	       AudioStreamIndex="0">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	      <Source
	          AudioStreamIndex="0"
	         MediaFile="%2%">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	     </Sources>
	</MediaFile>

This example stitches together portions of the second, the first, and the third input assets. Note that since each video is referenced by a zero-based index it is possible to stitch two videos together that have the same name. Once you have created a preset file you must do the following:
 
- Upload your assets
- Load the preset configuration
- Create a job
- Get a reference to the Media Services Encoder
- Create a task specifying the input assets, the preset configuration, the media encoder, and the output asset
- Submit the job

The following code snippet shows how to do these steps:
	
	static public void StitchWithMultipleAssets()
	{
        // Create and cache the Media Services credentials in a static class variable.
        _cachedCredentials = new MediaServicesCredentials(
                        MediaServicesAccountName,
                        MediaServicesAccountKey);
        // Used the cached credentials to create CloudMediaContext.
        _context = new CloudMediaContext(_cachedCredentials);
		
		// Upload assets to stitch
		IAsset inputAsset1 = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, video1.mp4);
		IAsset inputAsset2 = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, video2.wmv);
		IAsset inputAsset3 = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, video3.wmv);
		
		// Load the preset configuration
		string presetFileName = "StitchingWithMultipleAssets.xml";
		string configuration = File.ReadAllText(presetFileName);
		
		// Create a job
		IJob job = _context.Jobs.Create("A AME stitching job, using " + presetFileName);
		         
		// Get a reference to the media services encoder   
		IMediaProcessor processor = GetLatestMediaProcessorByName("Azure Media Encoder");
		    
		// Create a task    
		ITask task = job.Tasks.AddNew("Encode Task for stitching, using " + presetFileName, processor, configuration, TaskOptions.None);
		
		// Add the input assets
		task.InputAssets.Add(inputAsset1);
		task.InputAssets.Add(inputAsset2);
		task.InputAssets.Add(inputAsset3);
		
		// Add the output asset
		task.OutputAssets.AddNew("Result of a stitching job, using " + presetFileName, AssetCreationOptions.None);
		
		// Submit the job
		job.Submit();
		} 


This snippet loads each asset sequentially for simplicity. In production environments assets would be loaded in bulk. For more information on uploading multiple assets in bulk see [Ingesting Assets in Bulk with the Media Services SDK for .NET](media-services-dotnet-upload-files.md#ingest_in_bulk). For complete sample code see [Stitching with Media Services Encoder](https://code.msdn.microsoft.com/Stitching-with-Media-8fd5f203).

###Stitching Videos from a Single Asset

When stitching videos within a single asset, each video must have a unique name. The videos are specified using the MediaFile attribute using the filename as the attribute’s value. For example:
	
	<MediaFile
	    DeinterlaceMode="AutoPixelAdaptive"
	    ResizeQuality="Super"
	    AudioGainLevel="1"
	    VideoResizeMode="Stretch">
	    <Sources>
	      <Source
	        AudioStreamIndex="0"
	        MediaFile="video1.mp4">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	      <Source
	       AudioStreamIndex="0"
	       MediaFile="video2.wmv">
	
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	      <Source
	          AudioStreamIndex="0"
	         MediaFile="video3.wmv">
	        <Clips>
	          <Clip
	            StartTime="00:00:00"
	            EndTime="00:00:05" />
	        </Clips>
	      </Source>
	     </Sources>
	</MediaFile>

This preset stitches parts of video1.mp4, video2.wmv, and video3.wmv into the output asset.
The creation of the job and tasks is the same as stitching videos from multiple assets, you only need to upload a single asset as shown in the following code:

	// Creates a stitching job that uses a single asset 
    static public void StitchWithASingleAsset()
    {
        string presetFileName = "StitchingWithASingleAsset.xml";
        string configuration = File.ReadAllText(presetFileName);

        // Create and cache the Media Services credentials in a static class variable.
        _cachedCredentials = new MediaServicesCredentials(
                        MediaServicesAccountName,
                        MediaServicesAccountKey);
        // Used the cached credentials to create CloudMediaContext.
        _context = new CloudMediaContext(_cachedCredentials);

        IMediaProcessor processor = GetLatestMediaProcessorByName("Azure Media Encoder");
        IJob job = _context.Jobs.Create("A AME stitching job, using " + presetFileName);
        IAsset asset = CreateAssetAndUploadMultipleFiles(AssetCreationOptions.None, _stitchingFiles);

        ITask task = job.Tasks.AddNew("Encode Task for stitching, using " + presetFileName, processor, configuration, TaskOptions.None);
        task.InputAssets.Add(asset);
        task.OutputAssets.AddNew("Result of a stitching job, using " + presetFileName, AssetCreationOptions.None);

        job.Submit();
    }

##Encoding Presentations or Audio Streams With Mostly Speech
 
When encoding video whose audio track contains mostly speech, the default encoder presets may cause background noise to be amplified in the encoded asset. This behavior is caused by the NormalizeAudio attribute being set to true.

###Encoding Presentations with Mostly Speech

To prevent the amplification of background noise, do the following:

1. Copy the contents of the encoder preset you are using into an XML file. The encoder presets can be found at: Azure Media Encoder Schemas
1. Delete the NormalizeAudio attribute, it can be found near the top of the preset file under the <MediaFile> element:
	
	<MediaFile
	     DeinterlaceMode="AutoPixelAdaptive"
	     ResizeQuality="Super"
	     NormalizeAudio="True"
	     AudioGainLevel="1"
	     VideoResizeMode="Stretch">

1. Save the modified preset file to your local hard drive, and use code such as the following to encode with the custom preset:
		
		// Upload file and create asset
		IAsset asset = CreateAssetAndUploadSingleFile(AssetCreationOptions.None, @"C:\TEMP\Original.mp4");
		 
		string inputPresetFile = @"C:\TEMP\H264 Broadband 720p NoAudioNorm.xml";
		string presetName = Path.GetFileNameWithoutExtension(inputPresetFile);
		 
		IJob job = _context.Jobs.Create("Encode Job for " + asset.Name + ", encoded using " +  presetName);
		
		Console.WriteLine("Encode Job for " + asset.Name + ", encoded using " + presetName);
		
		// Get a media processor reference, and pass to it the name of the processor to use for the specific task.
		IMediaProcessor processor = GetLatestMediaProcessorByName("Azure Media Encoder");
		Console.WriteLine("Got MP " + processor.Name + ", ID : " + processor.Id + ", version: " + processor.Version);
		 
		// Read the configuration data into a string. 
		string configuration = File.ReadAllText(inputPresetFile);
		 
		// Create a task with the encoding details, using a string preset.
		ITask task = job.Tasks.AddNew("Encode Task for " + asset.Name + ", encoded using " + presetName, processor, configuration,
		                Microsoft.WindowsAzure.MediaServices.Client.TaskOptions.None);
		 
		// Specify the input asset to be encoded.
		task.InputAssets.Add(asset);
		 
		// Add an output asset to contain the results of the job.
		task.OutputAssets.AddNew("Output asset for " + asset.Name + ", encoded using " + presetName, AssetCreationOptions.None);
		 
		// Launch the job. 
		job.Submit();

##Media Services learning paths

You can view AMS learning paths here:

- [AMS Live Streaming Workflow](http://azure.microsoft.com/documentation/learning-paths/media-services-streaming-live/)
- [AMS on Demand Streaming Workflow](http://azure.microsoft.com/documentation/learning-paths/media-services-streaming-on-demand/)


##See Also

[Azure Media Encoder XML Schema](https://msdn.microsoft.com/library/azure/dn584702.aspx)