---
title: Model monitoring for generative AI applications (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI applications deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: prompt-flow
ms.reviewer: scottpolly
reviewer: s-polly
ms.topic: how-to
ms.date: 09/06/2023
ms.custom: devplatv2
---


# Model monitoring for generative AI applications (preview)

Monitoring models in production is an essential part of the AI lifecycle. Changes in data and consumer behavior can influence your generative AI application over time, resulting in outdated systems that negatively affect business outcomes and expose organizations to compliance, economic, and reputational risks. 

> [!IMPORTANT]
> Monitoring and Promptflow features are currently in public preview. These previews are provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).

Azure Machine Learning model monitoring for generative AI applications makes it easier for you to monitor your LLM applications in production for safety and quality on a cadence to ensure it's delivering maximum business impact. Monitoring ultimately helps maintain the quality and safety of your generative AI applications. Capabilities and integrations include: 
- Collect production data using [Model data collector](../concept-data-collection.md).
- [Responsible AI evaluation metrics](#evaluation-metrics) such as groundedness, coherence, fluency, relevance, and similarity, which are interoperable with [Azure Machine Learning prompt flow evaluation metrics](how-to-bulk-test-evaluate-flow.md).
- Ability to configure alerts for violations based on organizational targets and run monitoring on a recurring basis
- Consume results in a rich dashboard within a workspace in the Azure Machine Learning studio.
- Integration with Azure Machine Learning prompt flow evaluation metrics, analysis of collected production data to provide timely alerts, and visualization of the metrics over time. â€‹

For overall model monitoring basic concepts, refer to [Model monitoring with Azure Machine Learning (preview)](../concept-model-monitoring.md). In this article, you learn how to monitor a generative AI application backed by a managed online endpoint. The steps you take are:

- Configure [prerequisites](#prerequisites)
- [Create your monitor](#create-your-monitor)
- [Confirm monitoring status](#confirm-monitoring-status)
- [Consume monitoring results](#consume-results)


## Evaluation metrics
Metrics are generated by the following state-of-the-art GPT language models configured with specific evaluation instructions(prompt templates) which act as evaluator models for sequence-to-sequence tasks. This technique has shown strong empirical results and high correlation with human judgment when compared to standard generative AI evaluation metrics. Form more information about prompt flow evaluation, see [Submit bulk test and evaluate a flow (preview)](how-to-bulk-test-evaluate-flow.md) for more information about prompt flow evaluation. 

These GPT models are supported, and will be configured as your Azure OpenAI resource: 
* GPT-3.5 Turbo
* GPT-4
* GPT-4-32k  

The following metrics are supported. For more detailed information about each metric, see [Monitoring evaluation metrics descriptions and use cases](concept-model-monitoring-generative-ai-evaluation-metrics.md) 

- **Groundedness**:  evaluates how well the model's generated answers align with information from the input source. 
- **Relevance**:  evaluates the extent to which the model's generated responses are pertinent and directly related to the given questions. 
- **Coherence**: evaluates how well the language model can produce output flows smoothly, reads naturally, and resembles human-like language. 
- **Fluency**:  evaluates the language proficiency of a generative AI's predicted answer. It assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses. 
- **Similarity**:  evaluates the similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model.

    
#### Metric configuration requirements

The following inputs (data column names) are required to measure generation safety & quality: 
- **prompt text** - the original prompt given (also known as "inputs" or "question")
- **completion text** - the final completion from the API call that is returned (also known as "outputs" or "answer")
- **context text** - any context data that is sent to the API call, together with original prompt. For example, if you hope to get search results only from certain certified information sources/website, you can define in the evaluation steps. This is an optional step that can be configured through PromptFlow.
- **ground truth text** - the user-defined text as the "source of truth" (optional)

What parameters are configured in your data asset dictates what metrics you can produce, according to this table:  

|Metric  |Prompt  |Completion  |Context  |Ground truth  |
|---------|---------|---------|---------|---------|
|Coherence     |Required        |Required        |    -     |    -     |
|Fluency     |Required        |Required         |     -    |      -   |
|Groundedness     |Required        |Required         |Required         |     -    |
|Relevance     |Required       |Required         |Required         |     -    |
|Similarity     |Required        |Required         |   -      |Required         |


## Prerequisites
1. **Azure OpenAI resource:** You must have an Azure OpenAI resource created with sufficient quota. This resource is used as your evaluation endpoint. 
1. **Managed identity:**  Create a User Assigned managed Identity (UAI) and attach it to your workspace using the guidance in [Attach user assigned managed identity using CLI v2](../how-to-submit-spark-jobs.md#attach-user-assigned-managed-identity-using-cli-v2)with sufficient role access, as defined in the next step.
1. **Role access** To assign a role with the required permissions, you need to have the *owner* or *Microsoft.Authorization/roleAssignments/write* permission on your resource. Updating connections and permissions may take several minutes to take effect. These additional roles must be assigned to your UAI:
    - **Resource:** Workspace
    - **Role:** Azure Machine Learning Data Scientist
1. **Workspace connection:** [following this guidance](get-started-prompt-flow.md#connection), you use a managed identity that represents the credentials to the Azure OpenAI endpoint used to calculate the monitoring metrics. **DO NOT** delete the connection once it's used in the flow.
    -  **API version:** 2023-03-15-preview
1. **Prompt flow deployment:** Create a prompt flow runtime [following this guidance](how-to-create-manage-runtime.md), run your flow, and ensure your [deployment is configured using this article as a guide](how-to-deploy-for-real-time-inference.md) 
    - **Flow inputs & outputs:** You need to name your flow outputs appropriately and remember these column names when creating your monitor. In this article, we use the following:
        - **Inputs (required):** "prompt" 
        - **Outputs (required):** "completion" 
            - **Outputs (optional):** "context" | "ground truth" 
    - **Data collection:** in the "Deployment" _(Step #2 of the PromptFlow deployment wizard)_, the 'inference data collection' toggle must be enabled using [Model Data Collector](../concept-data-collection.md) 
    - **Outputs:** In the Outputs _(Step #3 of the PromptFlow deployment wizard)_, confirm you have selected the required outputs listed above (for example, completion | context | ground_truth) that meet your [metric configuration requirements](#metric-configuration-requirements) 

> [!NOTE]
> If your compute instance is behind a VNet, see [Network isolation in prompt flow](how-to-secure-prompt-flow.md).

## Create your monitor 

Create your monitor in the Monitoring overview page 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/create-monitor.png" alt-text="Screenshot showing how to create a monitor for your application." lightbox="./media/how-to-monitor-generative-ai-applications/create-monitor.png":::


### Configure basic monitoring settings

In the monitoring creation wizard, change **model task type** to **prompt & completion**, as shown by (A) in the screenshot.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/configure-basic-settings.png" alt-text="Screenshot showing how to configure basic monitoring settings for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/configure-basic-settings.png":::

### Configure data asset

If you have used [Model Data Collector](../concept-data-collection.md), select your two data assets (inputs & outputs). 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/configure-data-asset.png" alt-text="Screenshot showing how to configure your data asset for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/configure-data-asset.png":::
    
### Select monitoring signals

:::image type="content" source="./media/how-to-monitor-generative-ai-applications/configure-signal-2.png" alt-text="Screenshot showing monitoring signal configuration options on the monitoring settings dialog." lightbox="./media/how-to-monitor-generative-ai-applications/configure-signal-2.png":::
1. Configure workspace connection **(A)** in the screenshot. 
    1. You need to configure your workspace connection correctly, or you see this: 
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/configure-signal-1.png" alt-text="Screenshot showing an unconfigured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/configure-signal-1.png":::
1. Enter your Azure OpenAI evaluator deployment name **(B)**.
1. (Optional) Join your production data inputs & outputs: your production model inputs and outputs are automatically joined by the Monitoring service **(C)**. You can customize this if needed, but no action is required. By default, the join column is **correlationid**. 
1. (Optional) Configure metric thresholds: An acceptable per-instance score is fixed at 3/5. You can adjust your acceptable overall % passing rate between the range [1,99] % 

- Manually enter column names from your prompt flow **(E)**. Standard names are ("prompt" | "completion" | "context" | "ground_truth") but you can configure it according to your data asset. 
- (optional) Set sampling rate **(F)**

- Once configured, your signal will no longer show a warning.
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/configure-signal-3.png" alt-text="Screenshot showing monitoring signal configurations without a warning." lightbox="./media/how-to-monitor-generative-ai-applications/configure-signal-3.png":::

### Configure notifications

No action is required. You can configure more recipients if needed.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/configure-notifications.png" alt-text="Screenshot showing monitoring notification configurations." lightbox="./media/how-to-monitor-generative-ai-applications/configure-notifications.png":::

### Confirm monitoring signal configuration  

When successfully configured, your monitor should look like this:
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/confirm-configuration.png" alt-text="Screenshot showing a configured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/confirm-configuration.png":::

## Confirm monitoring status

If successfully configured, your monitoring pipeline job shows the following:
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/confirm-job-success.png" alt-text="Screenshot showing a successfully configured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/confirm-job-success.png":::

## Consume results  

### Monitor overview page

Your monitor overview provides an overview of your signal performance. You can enter your signal details page for more information.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/monitor-overview.png" alt-text="Screenshot showing monitor overview." lightbox="./media/how-to-monitor-generative-ai-applications/monitor-overview.png":::
    
### Signal details page

The signal details page allows you to view metrics over time **(A)**  and view histograms of distribution **(B)**.

:::image type="content" source="./media/how-to-monitor-generative-ai-applications/monitor-signal-details.png" alt-text="Screenshot showing a signal details page." lightbox="./media/how-to-monitor-generative-ai-applications/monitor-signal-details.png":::

### Resolve alerts 

It's only possible to adjust signal thresholds. The acceptable score is fixed at 3/5, and it's only possible to adjust the 'acceptable overall % passing rate' field. 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/monitor-signal-adjust-signal.png" alt-text="Screenshot adjusting signal thresholds." lightbox="./media/how-to-monitor-generative-ai-applications/monitor-signal-adjust-signal.png":::
   
## Next Steps
- [Model monitoring overview](../concept-model-monitoring.md)
- [Model data collector](../concept-data-collection.md)
- [Get started with Prompt flow](get-started-prompt-flow.md)
- [Submit bulk test and evaluate a flow (preview)](how-to-bulk-test-evaluate-flow.md)
- [Create evaluation flows](how-to-develop-an-evaluation-flow.md)
