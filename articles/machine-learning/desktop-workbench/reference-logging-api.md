---
title: Azure ML Logging API reference | Microsoft Docs
description: Logging API reference.
services: machine-learning
author: akshaya-a
ms.author: akannava
manager: mwinkle
ms.reviewer: garyericson, jasonwhowell, mldocs
ms.service: machine-learning
ms.component: core
ms.workload: data-services
ms.topic: article
ms.date: 09/25/2017

ROBOTS: NOINDEX
---


# Logging API reference

[!INCLUDE [workbench-deprecated](../../../includes/aml-deprecating-preview-2017.md)] 

Azure ML's logging library allows the program to emit metrics and files that are tracked by the history service for later analysis. 

## Uploading metrics

```python
# import logging API package
from azureml.logging import get_azureml_logger

# initialize a logger object
logger = get_azureml_logger()

# log "scalar" metrics
logger.log("simple integer value", 7)
logger.log("simple float value", 3.141592)
logger.log("simple string value", "this is a string metric")

# log a list of numerical values. 
# this automatically creates a chart in the Run History details page
logger.log("chart data points", [1, 3, 5, 10, 6, 4])
```

By default, all metrics are submitted asynchronously so that the submission doesn't impede program execution. This can cause ordering issues when multiple metrics are sent in edge cases. An example of this would be two metrics logged at the same time, but for some reason the user would prefer their exact ordering be preserved. Another case is when the metric must be tracked before running some code that is known to potentially fail fast. In both cases, the solution is to _wait_ until the metric is fully logged before proceeding:

```python
# blocking call
logger.log("my metric 1", 1).wait()
logger.log("my metric 2", 2).wait()
```

## Consuming metrics

The metrics are stored by the history service and tied to the Run that produced them. Both the Run History tab and CLI command below allow you to retrieve them (and artifacts below) after a run has completed.

```azurecli
# show the last run
$ az ml history last

# list all past runs
$ az ml history list 

# show a paritcular run
$ az ml history info -r <runid>
```

## Artifacts (files)

In addition to metrics, AzureML allows the user to track files as well. By default, all files written into the `outputs` folder relative to the program's working directory (the project folder in the compute context) are uploaded to the history service and tracked for later analysis. The caveat is that the individual file size must be smaller than 512 MB.


```Python
# Log content as an artifact
logger.upload("artifact/path", "This should be the contents of artifact/path in the service")
```

## Consuming artifacts

To print the contents of an artifact that has been tracked, user can use the Run History tab for the given run to **Download** or **Promote** the Artifact, or use the below CLI commands to achieve the same effect.

```azurecli
# show all artifacts generated by a run
$ az ml history info -r <runid> -a <artifact/path>

# promote a particular artifact
$ az ml history promote -r <runid> -ap <artifact/prefix> -n <name of asset to create>
```
## Next steps
- Walk through the [Classifying iris tutoria, part 2](tutorial-classifying-iris-part-2.md) to see logging API in action.
- Review [How to Use Run History and Model Metrics in Azure Machine Learning Workbench](how-to-use-run-history-model-metrics.md) to understand deeper how logging APIs can be used in Run History.
